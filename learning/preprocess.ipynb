{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11359,"status":"ok","timestamp":1687420979398,"user":{"displayName":"Yaad Luria","userId":"16702975035594285508"},"user_tz":-180},"id":"Hec1cryOeET1","outputId":"4b435701-ab6f-4ebe-eaba-882d74cdd2dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting Bio\n","  Downloading bio-1.5.9-py3-none-any.whl (276 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting biopython>=1.80 (from Bio)\n","  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.65.0)\n","Collecting mygene (from Bio)\n","  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.6.0)\n","Collecting gprofiler-official (from Bio)\n","  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.22.4)\n","Collecting biothings-client>=0.2.6 (from mygene->Bio)\n","  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2022.7.1)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n","Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n","Successfully installed Bio-1.5.9 biopython-1.81 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["# delete this cell if working on Pycharm\n","!pip install Bio\n","!pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32587,"status":"ok","timestamp":1687421011970,"user":{"displayName":"Yaad Luria","userId":"16702975035594285508"},"user_tz":-180},"id":"AzMza1WGsqZu","outputId":"bf6e9aaf-9a93-4ec4-f566-c507379dc72b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mm2sUeKhcGEv"},"outputs":[],"source":["from Bio.PDB import *\n","import numpy as np\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9k0WEa0cNO0"},"outputs":[],"source":["#Change Here\n","NB_MAX_LENGTH = 140\n","AA_DICT = {\"A\": 0, \"C\": 1, \"D\": 2, \"E\": 3, \"F\": 4, \"G\": 5, \"H\": 6, \"I\": 7, \"K\": 8, \"L\": 9, \"M\": 10, \"N\": 11,\n","           \"P\": 12, \"Q\": 13, \"R\": 14, \"S\": 15, \"T\": 16, \"W\": 17, \"Y\": 18, \"V\": 19, \"X\": 20, \"-\": 21}\n","FEATURE_NUM = len(AA_DICT)\n","BACKBONE_ATOMS = [\"N\", \"CA\", \"C\", \"O\", \"CB\"]\n","OUTPUT_SIZE = len(BACKBONE_ATOMS) * 3\n","NB_CHAIN_ID = \"H\"\n"]},{"cell_type":"code","source":["def check_valid_protein_sequence(seq):\n","    amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n","\n","    for aa in seq:\n","        if aa.upper() not in amino_acids:\n","            return f\"Invalid amino acid: {aa}\"\n","\n","    return \"Valid protein sequence!\""],"metadata":{"id":"ICcEEvxPbxY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNxeyEawcZ9y"},"outputs":[],"source":["def get_seq_aa(pdb_file, chain_id): #Stays the same\n","    \"\"\"\n","    returns the sequence (String) and a list of all the aa residue objects of the given protein chain.\n","    :param pdb_file: path to a pdb file\n","    :param chain_id: chain letter (char)\n","    :return: sequence, [aa objects]\n","    \"\"\"\n","    # load model\n","    chain = PDBParser(QUIET=True).get_structure(pdb_file, pdb_file)[0][chain_id]\n","\n","    aa_residues = []\n","    seq = \"\"\n","\n","    for residue in chain.get_residues():\n","        aa = residue.get_resname()\n","        if not is_aa(aa) or not residue.has_id('CA'): # Not amino acid\n","            continue\n","        elif aa == \"UNK\":  # unkown amino acid\n","            seq += \"X\"\n","        else:\n","            seq += Polypeptide.three_to_one(residue.get_resname())\n","        aa_residues.append(residue)\n","\n","    return seq, aa_residues"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNh1JDxtcgnF"},"outputs":[],"source":["def generate_input_one_hot(pdb_file): # TODO: implement this!\n","    \"\"\"\n","    receives a pdb file and returns its sequence in a one-hot encoding matrix (each row is an aa in the sequence, and\n","    each column represents a different aa out of the 20 aa + 2 special columns).\n","    :param pdb_file: path to a pdb file (nanobody, heavy chain has id 'H')\n","    :return: numpy array of shape (NB_MAX_LENGTH, FEATURE_NUM)\n","    \"\"\"\n","\n","    # get seq and aa residues\n","    seq, _ = get_seq_aa(pdb_file, NB_CHAIN_ID)\n","\n","    # TODO: fill the missing code lines.\n","    # create one-hot encoding matrix\n","    one_hot = np.zeros((NB_MAX_LENGTH, FEATURE_NUM))\n","\n","    # fill the matrix\n","    for i, aa in enumerate(seq):\n","        one_hot[i, AA_DICT[aa]] = 1\n","\n","    # pad the matrix\n","    for i in range(len(seq), NB_MAX_LENGTH):\n","        one_hot[i, AA_DICT[\"-\"]] = 1\n","\n","    return one_hot\n"]},{"cell_type":"code","source":["# struct that will hold possible embeddings dimensions, for example name \"5120\", value 5120 dimensions...\n","possible_embedding_dims = { 1280, 640, 480, 320}\n","# dict of embedding dimensions to their corresponding model\n","embedding_dim_to_model = {\n","    1280: \"esm2_t33_650M_UR50D\",\n","    640: \"esm2_t30_150M_UR50D\",\n","    480: \"esm2_t12_35M_UR50D\",\n","    320: \"esm2_t6_8M_UR50D\",\n","}\n","\n","# https://github.com/facebookresearch/esm\n","# Load ESM-2 model\n","import torch\n","embedding_dim = 1280  # change to change the modle and then run this block!\n","alphabet = None\n","model = None\n","\n","# load pretrained model, according to the embedding dimension\n","if embedding_dim == 1280:\n","    model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm2_t33_650M_UR50D\")\n","    last_layer = 33\n","elif embedding_dim == 640:\n","      model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm2_t30_150M_UR50D\")\n","      last_layer = 30\n","elif embedding_dim == 480:\n","      model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm2_t12_35M_UR50D\")\n","      last_layer = 12\n","elif embedding_dim == 320:\n","      model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm2_t6_8M_UR50D\")\n","      last_layer = 6\n","\n","# update FEATURE_NUM\n","FEATURE_NUM = embedding_dim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JytY0tgeuzww","executionInfo":{"status":"ok","timestamp":1687421705203,"user_tz":-180,"elapsed":21000,"user":{"displayName":"Yaad Luria","userId":"16702975035594285508"}},"outputId":"079d19d5-5338-484e-90f7-4575579d2dec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_esm_main\n","Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D.pt\n","Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t33_650M_UR50D-contact-regression.pt\n"]}]},{"cell_type":"code","source":["def get_esm_embedding_for_protein_sequence(sequence):\n","    global alphabet, model\n","    \"\"\"\n","    Receives a protein sequence (String) and returns the ESM representation of the sequence (numpy array).\n","    :param sequence: protein sequence (String)\n","    :return: ESM representation of the sequence (numpy array)\n","    \"\"\"\n","    model.eval()  # disables dropout for deterministic results\n","    batch_converter = alphabet.get_batch_converter()\n","\n","\n","    # Prepare data (protein_1, seq_1), (protein_2, seq_2), ...\n","    data = [(\"protein\", sequence) for i, seq in enumerate(sequence)]\n","    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n","    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n","\n","\n","    # Extract per-residue embeddings (on CPU)\n","    with torch.no_grad():\n","        results = model(batch_tokens, repr_layers=[last_layer], return_contacts=False)\n","    token_embeddings = results[\"representations\"][last_layer]\n","\n","    # Generate per-sequence embeddings via averaging\n","    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n","\n","    sequence_representations = []\n","    for i, tokens_len in enumerate(batch_lens):\n","        sequence_representations.append(token_embeddings[i, 1 : tokens_len - 1, :])\n","\n","    emb = np.zeros((len(sequence_representations), 140, embedding_dim))\n","    for i, mat in enumerate(sequence_representations):\n","      s, _ = mat.size()\n","      emb[i, :s, :] = mat\n","\n","    return emb\n"],"metadata":{"id":"mdRDO-zIzFEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WijV_pznwoET"},"outputs":[],"source":["\n","# will generate ESM embeddings for proteins sequences and return it in the shape of (NB_MAX_LENGTH, EMBEDDING_DIM)\n","def generate_input_embedding(seq):\n","    \"\"\"\n","    receives a pdb file and embedding dimension and returns its embedding in a matrix of shape (NB_MAX_LENGTH, embedding_dim).\n","    :param pdb_file: path to a pdb file (nanobody, heavy chain has id 'H')\n","    :param embedding_dim: the dimension of the embedding\n","    :return: numpy array of shape (NB_MAX_LENGTH, embedding_dim)\n","    \"\"\"\n","\n","\n","    batch_converter = alphabet.get_batch_converter()\n","    model.eval()  # disables dropout for deterministic results\n","    model.to(device)\n","\n","    # Prepare data (protein_1, seq_1), (protein_2, seq_2), ...\n","    data = [(f\"protein_{i}\", seq) for i, seq in enumerate(seq)]\n","    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n","    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n","\n","\n","    # Extract per-residue embeddings (on CPU)\n","    with torch.no_grad():\n","        results = model(batch_tokens.to(device), repr_layers=[last_layer], return_contacts=False)\n","    token_embeddings = results[\"representations\"][last_layer]\n","\n","    # Generate per-sequence embeddings via averaging\n","    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n","\n","    sequence_representations = []\n","    for i, tokens_len in enumerate(batch_lens):\n","        sequence_representations.append(token_embeddings[i, 1 : tokens_len - 1, :].cpu())\n","\n","    return sequence_representations"]},{"cell_type":"code","source":[" def generate_input_embedding_contact(seq):\n","    \"\"\"\n","    receives a pdb file and embedding dimension and returns its embedding in a matrix of shape (NB_MAX_LENGTH, embedding_dim).\n","    :param pdb_file: path to a pdb file (nanobody, heavy chain has id 'H')\n","    :param embedding_dim: the dimension of the embedding\n","    :return: numpy array of shape (NB_MAX_LENGTH, embedding_dim)\n","    \"\"\"\n","\n","\n","    batch_converter = alphabet.get_batch_converter()\n","    model.eval()  # disables dropout for deterministic results\n","    model.to(device)\n","\n","    # Prepare data (protein_1, seq_1), (protein_2, seq_2), ...\n","    data = [(f\"protein_{i}\", seq) for i, seq in enumerate(seq)]\n","    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n","    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n","\n","\n","    # Extract per-residue embeddings (on CPU)\n","    with torch.no_grad():\n","        results = model(batch_tokens.to(device), repr_layers=[last_layer], return_contacts=True)\n","    token_embeddings = results[\"representations\"][last_layer]\n","\n","    # Generate per-sequence embeddings via averaging\n","    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n","\n","    sequence_representations = []\n","    for i, tokens_len in enumerate(batch_lens):\n","        sequence_representations.append(token_embeddings[i, 1 : tokens_len - 1, :].cpu())\n","\n","    # load contact maps\n","    maps = []\n","    for (_, seq), tokens_len, attention_contacts in zip(data, batch_lens, results[\"contacts\"]):\n","      cut_contacts = attention_contacts[: tokens_len, : tokens_len][1:-1, 1:-1]\n","      maps.append(cut_contacts)\n","\n","    # return the embeddings\n","    return sequence_representations, maps"],"metadata":{"id":"J-Z0H1cEzhHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5G7dbbccolV"},"outputs":[],"source":["def generate_label(pdb_file):  # Stays the same\n","    \"\"\"\n","    receives a pdb file and returns its backbone + CB coordinates.\n","    :param pdb_file: path to a pdb file (nanobody, heavy chain has id 'H') already alingned to a reference nanobody.\n","    :return: numpy array of shape (CDR_MAX_LENGTH, OUTPUT_SIZE).\n","    \"\"\"\n","    # get seq and aa residues\n","    seq, aa_residues = get_seq_aa(pdb_file, NB_CHAIN_ID)\n","\n","    # create empty matrix\n","    label = np.zeros((NB_MAX_LENGTH, OUTPUT_SIZE))\n","\n","    # fill the matrix with the backbone + CB coordinates\n","    for i, residue in enumerate(aa_residues):\n","        for j, atom in enumerate(BACKBONE_ATOMS):\n","            # check if atom in backbone\n","            if not residue.has_id(atom):\n","                continue\n","            label[i, j * 3:j * 3 + 3] = residue[atom].get_coord()\n","\n","    return label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1p6szo4hKjY5"},"outputs":[],"source":["def matrix_to_pdb(seq, coord_matrix, pdb_name):\n","    \"\"\"\n","    Receives a sequence (String) and the output matrix of the neural network (coord_matrix, numpy array)\n","    and creates from them a PDB file named pdb_name.pdb.\n","    :param seq: protein sequence (String), with no padding\n","    :param coord_matrix: output np array of the nanobody neural network, shape = (NB_MAX_LENGTH, OUTPUT_SIZE)\n","    :param pdb_name: name of the output PDB file (String)\n","    \"\"\"\n","    ATOM_LINE = \"ATOM{}{}  {}{}{} {}{}{}{}{:.3f}{}{:.3f}{}{:.3f}  1.00{}{:.2f}           {}\\n\"\n","    END_LINE = \"END\\n\"\n","    k = 1\n","    with open(f\"{pdb_name}.pdb\", \"w\") as pdb_file:\n","        for i, aa in enumerate(seq):\n","            third_space = (4 - len(str(i))) * \" \"\n","            for j, atom in enumerate(BACKBONE_ATOMS):\n","                if not (aa == \"G\" and atom == \"CB\"):  # GLY lacks CB atom\n","                    x, y, z = coord_matrix[i][3*j], coord_matrix[i][3*j+1], coord_matrix[i][3*j+2]\n","                    b_factor = 0.00\n","                    first_space = (7 - len(str(k))) * \" \"\n","                    second_space = (4 - len(atom)) * \" \"\n","                    forth_space = (12 - len(\"{:.3f}\".format(x))) * \" \"\n","                    fifth_space = (8 - len(\"{:.3f}\".format(y))) * \" \"\n","                    sixth_space = (8 - len(\"{:.3f}\".format(z))) * \" \"\n","                    seventh_space = (6 - len(\"{:.2f}\".format(b_factor))) * \" \"\n","\n","                    pdb_file.write(ATOM_LINE.format(first_space, k, atom, second_space, Polypeptide.one_to_three(aa) , \"H\", third_space,\n","                                                    i, forth_space, x, fifth_space, y, sixth_space, z, seventh_space,\n","                                                    b_factor, atom[0]))\n","                    k += 1\n","\n","        pdb_file.write(END_LINE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5Gxoyt_cowc"},"outputs":[],"source":["# if __name__ == '__main__':\n","\n","#    #  you can make all the data for the network in this section.\n","#    # you can save the matrices to your drive and load them in your google colab file later.\n","\n","#     device = torch.device(\"cuda\")\n","#     input_matrix = None\n","#     input_tesnors = []\n","#     labels_matrix = []\n","#     data_path = \"/content/drive/MyDrive/Hackathon3D/Datasets/all_data\"  # TODO: change path if needed\n","\n","#     # creat list of all the protein sequences\n","#     sequences = []\n","#     for pdb in tqdm(os.listdir(data_path)):\n","#         seq, _ = get_seq_aa(os.path.join(data_path, pdb), NB_CHAIN_ID)\n","#         sequences.append(seq)\n","\n","#     # save input matrices, send 100 sequences at a time to generate_input_embedding function\n","#     batch_size = 100\n","#     embeddings = None\n","#     for i in tqdm(range(0, len(sequences), batch_size)):\n","#         if embeddings == None:\n","#             embeddings = generate_input_embedding(sequences[i:i + batch_size])\n","#         else:\n","#           embeddings += generate_input_embedding(sequences[i:i + batch_size])\n","\n","#     # save labels\n","#     for pdb in tqdm(os.listdir(data_path)):\n","#         nb_xyz = generate_label(os.path.join(data_path, pdb))\n","#         labels_matrix.append(nb_xyz)\n","\n","#     save_path = \"/content/drive/MyDrive/Hackathon3D/Training_inputs_labels\"  # TODO: change path if needed\n","\n","#     np.save(f\"{save_path}/train_labels.npy\", np.array(labels_matrix))\n","\n","\n","# # make npy from tenors file, padd the sequences to the size of embedding_dim\n","# emb = np.zeros((len(embeddings), 140, embedding_dim))\n","# for i, mat in enumerate(embeddings):\n","#   s, _ = mat.size()\n","#   emb[i, :s, :] = mat\n","\n","# np.save(f\"{save_path}/train_input.npy\", np.array(emb))\n","\n","# print(emb.shape)\n","\n","# with contact maps version\n","if __name__ == '__main__':\n","\n","   #  you can make all the data for the network in this section.\n","   # you can save the matrices to your drive and load them in your google colab file later.\n","\n","    device = torch.device(\"cuda\")\n","    input_matrix = None\n","    input_tesnors = []\n","    labels_matrix = []\n","    data_path = \"/content/drive/MyDrive/Hackathon3D/Datasets/all_data\"  # TODO: change path if needed\n","    save_path = \"/content/drive/MyDrive/Hackathon3D/Training_inputs_labels/multi\"  # TODO: change path if needed\n","\n","    # creat list of all the protein sequences\n","    sequences = []\n","    for pdb in tqdm(os.listdir(data_path)):\n","        seq, _ = get_seq_aa(os.path.join(data_path, pdb), NB_CHAIN_ID)\n","        sequences.append(seq)\n","\n","    # save input matrices, send 100 sequences at a time to generate_input_embedding function\n","    batch_size = 20\n","    embeddings = None\n","    maps = None\n","    for i in tqdm(range(0, len(sequences), batch_size)):\n","        if embeddings == None:\n","            embeddings, maps = generate_input_embedding_contact(sequences[i:i + batch_size])\n","        else:\n","          new_embeddings, new_maps = generate_input_embedding_contact(sequences[i:i + batch_size])\n","          embeddings += new_embeddings\n","          maps += new_maps\n","\n","    # # save labels- run only once and use for all models\n","    # for pdb in tqdm(os.listdir(data_path)):\n","    #     nb_xyz = generate_label(os.path.join(data_path, pdb))\n","    #     labels_matrix.append(nb_xyz)\n","    # np.save(f\"{save_path}/train_labels.npy\", np.array(labels_matrix))\n","\n","\n","    # make npy from tenors file, padd the sequences to the size of embedding_dim\n","    emb = np.zeros((len(embeddings), 140, embedding_dim))\n","    for i, mat in enumerate(embeddings):\n","      s, _ = mat.size()\n","      emb[i, :s, :] = mat.cpu()\n","\n","\n","    # same for contact maps\n","    con = np.zeros((len(maps), 140, 140))\n","    for i, mat in enumerate(maps):\n","      s, t = mat.size()\n","      con[i, :s, :t] = mat.cpu()\n","\n","    np.save(f\"{save_path}/embeddings_{FEATURE_NUM}.npy\", np.array(emb))\n","    np.save(f\"{save_path}/contacts_{FEATURE_NUM}.npy\", np.array(con))\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"14MEmGGblX1MERq08Q06tQP_pERgEGNl3","timestamp":1618651974479}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}